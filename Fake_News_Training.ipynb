{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ilkayyagizgur/Fake_News_Training/blob/main/Fake_News_Training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k22fnkk9f-Js"
      },
      "source": [
        "# Download and Import Required Libraries\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall -y transformers accelerate\n",
        "!pip install transformers accelerate"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 763
        },
        "id": "Skjq-Hm1A0Yt",
        "outputId": "929a8be5-906d-4ac4-fd99-44b72ccc9262"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: transformers 4.29.2\n",
            "Uninstalling transformers-4.29.2:\n",
            "  Successfully uninstalled transformers-4.29.2\n",
            "Found existing installation: accelerate 0.19.0\n",
            "Uninstalling accelerate-0.19.0:\n",
            "  Successfully uninstalled accelerate-0.19.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Using cached transformers-4.29.2-py3-none-any.whl (7.1 MB)\n",
            "Collecting accelerate\n",
            "  Using cached accelerate-0.19.0-py3-none-any.whl (219 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.14.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.13.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.0.1+cu118)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->accelerate) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->accelerate) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->accelerate) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->accelerate) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.6.0->accelerate) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.6.0->accelerate) (16.0.5)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.6.0->accelerate) (2.1.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.6.0->accelerate) (1.3.0)\n",
            "Installing collected packages: transformers, accelerate\n",
            "Successfully installed accelerate-0.19.0 transformers-4.29.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "transformers"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z7W7dPhloq1t",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ecd03d31-c96c-4ab8-fa35-07417f7ca4de"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.29.2-py3-none-any.whl (7.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.1/7.1 MB\u001b[0m \u001b[31m55.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.0)\n",
            "Collecting huggingface-hub<1.0,>=0.14.1 (from transformers)\n",
            "  Downloading huggingface_hub-0.14.1-py3-none-any.whl (224 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m78.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.14.1 tokenizers-0.13.3 transformers-4.29.2\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting datasets\n",
            "  Downloading datasets-2.12.0-py3-none-any.whl (474 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m474.6/474.6 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.22.4)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (9.0.0)\n",
            "Collecting dill<0.3.7,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.6-py3-none-any.whl (110 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.27.1)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.65.0)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.5/212.5 kB\u001b[0m \u001b[31m26.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multiprocess (from datasets)\n",
            "  Downloading multiprocess-0.70.14-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.3/134.3 kB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.4.0)\n",
            "Collecting aiohttp (from datasets)\n",
            "  Downloading aiohttp-3.8.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m57.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: huggingface-hub<1.0.0,>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.14.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.1)\n",
            "Collecting responses<0.19 (from datasets)\n",
            "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.0.12)\n",
            "Collecting multidict<7.0,>=4.5 (from aiohttp->datasets)\n",
            "  Downloading multidict-6.0.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.5/114.5 kB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting async-timeout<5.0,>=4.0.0a3 (from aiohttp->datasets)\n",
            "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
            "Collecting yarl<2.0,>=1.0 (from aiohttp->datasets)\n",
            "  Downloading yarl-1.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (268 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m30.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting frozenlist>=1.1.1 (from aiohttp->datasets)\n",
            "  Downloading frozenlist-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (149 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.6/149.6 kB\u001b[0m \u001b[31m19.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting aiosignal>=1.1.2 (from aiohttp->datasets)\n",
            "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets) (3.12.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2022.7.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
            "Installing collected packages: xxhash, multidict, frozenlist, dill, async-timeout, yarl, responses, multiprocess, aiosignal, aiohttp, datasets\n",
            "Successfully installed aiohttp-3.8.4 aiosignal-1.3.1 async-timeout-4.0.2 datasets-2.12.0 dill-0.3.6 frozenlist-1.3.3 multidict-6.0.4 multiprocess-0.70.14 responses-0.18.0 xxhash-3.2.0 yarl-1.9.2\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.22.4)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting evaluate\n",
            "  Downloading evaluate-0.4.0-py3-none-any.whl (81 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.4/81.4 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.12.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from evaluate) (1.22.4)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.3.6)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from evaluate) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.27.1)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from evaluate) (4.65.0)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from evaluate) (3.2.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.70.14)\n",
            "Requirement already satisfied: fsspec[http]>=2021.05.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2023.4.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.14.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from evaluate) (23.1)\n",
            "Requirement already satisfied: responses<0.19 in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.18.0)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (9.0.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.8.4)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.7.0->evaluate) (3.12.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2022.7.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->evaluate) (1.16.0)\n",
            "Installing collected packages: evaluate\n",
            "Successfully installed evaluate-0.4.0\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers\n",
        "!pip install datasets\n",
        "!pip install numpy\n",
        "!pip install evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "McjPNxWUqksI"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForSequenceClassification\n",
        "from transformers import TrainingArguments, Trainer\n",
        "from transformers import AutoTokenizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from datasets import Dataset\n",
        "import pyarrow as pa\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import evaluate\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "doWGJgStfsPP",
        "outputId": "319aa257-dd4f-4aeb-b157-f0a61aeef233"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "epaoMhSViA4F"
      },
      "source": [
        "# Model Settings\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UsuMsfOppjT-"
      },
      "outputs": [],
      "source": [
        "choosen_model ='dbmdz/bert-base-turkish-cased'\n",
        "number_labels = 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0JLURHyyiE4a"
      },
      "outputs": [],
      "source": [
        "dataset_location ='/content/drive/MyDrive/Bitirme/LastDatasetWithAllAug.csv'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CorMCqxvl7Qw"
      },
      "outputs": [],
      "source": [
        "trainer_output_dir = \"/content/drive/MyDrive/Berturk-cased-model\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "66LJW-SAlnsq"
      },
      "outputs": [],
      "source": [
        "saving_location = '/content/drive/MyDrive/Bert/Models/Berturk-cased-model'\n",
        "loading_location = saving_location"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "enlJ4sTVgI9W"
      },
      "source": [
        "# Choose model and Download\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UmMqbFbnqVUK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d01507d1-42d4-4092-8095-22719b5b4a96"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at dbmdz/bert-base-turkish-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(choosen_model)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    choosen_model,\n",
        "    num_labels=number_labels\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G3CpKBUbhQFq"
      },
      "outputs": [],
      "source": [
        "metrics = evaluate.combine([\"accuracy\", \"f1\", \"precision\", \"recall\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zDEBimJhha16"
      },
      "source": [
        "# Defining Required Functions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-J4CLweAhg3i"
      },
      "outputs": [],
      "source": [
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    return metrics.compute(predictions=predictions, references=labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4i-hpCuhhiAL"
      },
      "outputs": [],
      "source": [
        "def process_data(row):\n",
        "\n",
        "    text = row['Orj_Text']\n",
        "    text = str(text)\n",
        "    text = ' '.join(text.split())\n",
        "\n",
        "    encodings = tokenizer(text,\n",
        "                          padding=\"max_length\",\n",
        "                          truncation=True,\n",
        "                          max_length=128)\n",
        "\n",
        "    label = 0\n",
        "    if row['label'] == 1:\n",
        "        label += 1\n",
        "\n",
        "    encodings['labels'] = label\n",
        "    encodings['text'] = text\n",
        "\n",
        "    return encodings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-uYQgemKgSbD"
      },
      "source": [
        "# Import and Procces Dataset\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZGCG5G8SqVhr",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "28a5c179-e1d5-4dba-fac5-9982dff87d62"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   Unnamed: 0                                           Orj_Text  label\n",
              "0           0  ABD Dışişleri Bakanlığı Sözcüsü Toner, \"NATO m...      1\n",
              "1           1  Almanya Başbakanı Merkel, İstanbul'daki terör ...      1\n",
              "2           2  BERLİN\\nGeçen yılı terör korkusu, cumhurbaşkan...      1\n",
              "3           3  SAO PAULO\\nBrezilya'da, yeni yıl gecesi eski e...      1\n",
              "4           4  RAMALLAH\\nFilistin Devlet Başkanı Mahmud Abbas...      1"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-c5923427-5eb0-457d-be3a-99b4963afdc1\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>Orj_Text</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>ABD Dışişleri Bakanlığı Sözcüsü Toner, \"NATO m...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>Almanya Başbakanı Merkel, İstanbul'daki terör ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>BERLİN\\nGeçen yılı terör korkusu, cumhurbaşkan...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>SAO PAULO\\nBrezilya'da, yeni yıl gecesi eski e...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>RAMALLAH\\nFilistin Devlet Başkanı Mahmud Abbas...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c5923427-5eb0-457d-be3a-99b4963afdc1')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-c5923427-5eb0-457d-be3a-99b4963afdc1 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-c5923427-5eb0-457d-be3a-99b4963afdc1');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "dataset = pd.read_csv(dataset_location)\n",
        "dataset.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gQlzPgY-ikDM"
      },
      "outputs": [],
      "source": [
        "processed_dataset = []\n",
        "\n",
        "for i in range(len(dataset[:20037])):\n",
        "    processed_dataset.append(process_data(dataset.iloc[i]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zlbTLVZcSnQW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1b55918f-64b3-45e2-c145-9680d544f572"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_ids': [2, 3077, 6405, 4184, 15029, 19286, 1972, 16, 6, 9194, 16151, 2177, 2319, 11, 13539, 28457, 2362, 16390, 3844, 8138, 18, 2123, 2784, 20346, 16, 2701, 2319, 9778, 3146, 28457, 2362, 14632, 2680, 2638, 8423, 24512, 12752, 2055, 2077, 18, 6, 2859, 18, 59, 4095, 1052, 29047, 17271, 1037, 3077, 6405, 4184, 15029, 7315, 19286, 1972, 16, 2673, 11, 2054, 4418, 2665, 4024, 27880, 3609, 8139, 3716, 2178, 13161, 1985, 3075, 6857, 3284, 18106, 2073, 5034, 18, 31932, 19286, 1972, 16, 5236, 1996, 3254, 5247, 2673, 11, 2054, 4418, 2665, 4024, 17997, 2538, 14705, 5139, 7340, 18, 2178, 14797, 1992, 16067, 5388, 9465, 2208, 21884, 22553, 19318, 1007, 2353, 15703, 19286, 1972, 16, 6955, 5920, 30, 6, 26706, 3716, 2178, 13161, 1985, 3075, 11600, 3844, 10099, 18, 3], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': 1, 'text': 'ABD Dışişleri Bakanlığı Sözcüsü Toner, \"NATO müttefikimiz Türkiye\\'yle teröre karşı dayanışma halindeyiz. Bu tür saldırılar, sadece Türkiye hükümetiyle teröre karşı mücadelede birlikte çalışma kararlılığımızı artırmaktadır.\" dedi. WASHINGTON ABD Dışişleri Bakanlığı Sözcüsü Mark Toner, İstanbul\\'da meydana gelen terör saldırısına ilişkin soruşturma boyunca Türk yetkililerle yakın ilişki içerisinde olacaklarını belirtti. Sözcü Toner, yazılı bir açıklama yaparak İstanbul\\'da meydana gelen terör saldırısını kınadıklarını bildirdi. Türk halkına ve olayda hayatını kaybedenlerin yakınlarına başsağlığı dilediklerini aktaran Toner, şunları kaydetti: \"Soruşturma boyunca Türk yetkililerle yakın irtibat halinde olacağız. NATO müttefikimiz Türkiye\\'yle teröre karşı dayanışma halindeyiz. Ne yazık ki bu hain saldırı, masum insanları hedef alan en son çabadır. Bu tür saldırılar, sadece Türkiye hükümetiyle teröre karşı mücadelede birlikte çalışma kararlılığımızı artırmaktadır.\" Muhabir: Muhammed Bilal Kenasari'}"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "processed_dataset[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HhAC4hl-ipTi"
      },
      "outputs": [],
      "source": [
        "new_df = pd.DataFrame(processed_dataset)\n",
        "\n",
        "train_df, valid_df = train_test_split(\n",
        "    new_df,\n",
        "    test_size=0.2,\n",
        "    random_state=42\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_MHZHWCHjL4Z"
      },
      "outputs": [],
      "source": [
        "train_dataset = Dataset(pa.Table.from_pandas(train_df))\n",
        "valid_dataset = Dataset(pa.Table.from_pandas(valid_df))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IDm-QWI1jcqP"
      },
      "source": [
        "# Setting Up Trainer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OGnoBt1_jf1T"
      },
      "outputs": [],
      "source": [
        "training_args = TrainingArguments(output_dir=trainer_output_dir, evaluation_strategy=\"epoch\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PLCN1qVajh4c"
      },
      "outputs": [],
      "source": [
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=valid_dataset,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wwfNf4vSjtLK"
      },
      "source": [
        "# Train and Evaluate\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BAZb5U2cjx1k",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 259
        },
        "outputId": "2d709c50-2a11-4300-f78f-f63b13ed8e0c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='3612' max='3612' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [3612/3612 06:20, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>F1</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.084100</td>\n",
              "      <td>0.042112</td>\n",
              "      <td>0.990029</td>\n",
              "      <td>0.990244</td>\n",
              "      <td>0.983845</td>\n",
              "      <td>0.996727</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.018500</td>\n",
              "      <td>0.007741</td>\n",
              "      <td>0.998754</td>\n",
              "      <td>0.998771</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.997545</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.005000</td>\n",
              "      <td>0.012650</td>\n",
              "      <td>0.997507</td>\n",
              "      <td>0.997549</td>\n",
              "      <td>0.995922</td>\n",
              "      <td>0.999182</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=3612, training_loss=0.060426499888482414, metrics={'train_runtime': 383.6528, 'train_samples_per_second': 75.279, 'train_steps_per_second': 9.415, 'total_flos': 1899727597463040.0, 'train_loss': 0.060426499888482414, 'epoch': 3.0})"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2wJbxaXIjxkG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "outputId": "8d20d099-bbc6-4781-f835-ed87bb7ff2ff"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='301' max='301' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [301/301 00:06]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'eval_loss': 0.012650213204324245,\n",
              " 'eval_accuracy': 0.997507270461155,\n",
              " 'eval_f1': 0.997549019607843,\n",
              " 'eval_precision': 0.9959216965742251,\n",
              " 'eval_recall': 0.9991816693944353,\n",
              " 'eval_runtime': 6.6488,\n",
              " 'eval_samples_per_second': 362.021,\n",
              " 'eval_steps_per_second': 45.271,\n",
              " 'epoch': 3.0}"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "trainer.evaluate()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lSyIv6kIj3LY"
      },
      "source": [
        "# Save Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_nueQa25j6BY"
      },
      "outputs": [],
      "source": [
        "model.save_pretrained(saving_location)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.save_pretrained(saving_location, legacy_format=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8vs-8XzxCLDV",
        "outputId": "efc1ed1c-bd11-4a87-f1a0-1a632b89776a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('/content/drive/MyDrive/Bert/Models/Berturk-cased-model/tokenizer_config.json',\n",
              " '/content/drive/MyDrive/Bert/Models/Berturk-cased-model/special_tokens_map.json',\n",
              " '/content/drive/MyDrive/Bert/Models/Berturk-cased-model/tokenizer.json')"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BJE8Bc25kPgi"
      },
      "source": [
        "# Load Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QFkZwtcgkRYk",
        "outputId": "78fd6d47-5185-4a71-948c-d07b7a7ed3a7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loading configuration file /content/drive/MyDrive/Bert/Models/Berturk-cased/config.json\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"/content/drive/MyDrive/Bert/Models/Berturk-cased\",\n",
            "  \"architectures\": [\n",
            "    \"BertForSequenceClassification\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"problem_type\": \"single_label_classification\",\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.26.0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32000\n",
            "}\n",
            "\n",
            "loading weights file /content/drive/MyDrive/Bert/Models/Berturk-cased/pytorch_model.bin\n",
            "All model checkpoint weights were used when initializing BertForSequenceClassification.\n",
            "\n",
            "All the weights of BertForSequenceClassification were initialized from the model checkpoint at /content/drive/MyDrive/Bert/Models/Berturk-cased.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--dbmdz--bert-base-turkish-cased/snapshots/bd38a3ecfe5d183400a573b1903e940d8d34902b/config.json\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"dbmdz/bert-base-turkish-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.26.0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32000\n",
            "}\n",
            "\n",
            "loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--dbmdz--bert-base-turkish-cased/snapshots/bd38a3ecfe5d183400a573b1903e940d8d34902b/vocab.txt\n",
            "loading file tokenizer.json from cache at None\n",
            "loading file added_tokens.json from cache at None\n",
            "loading file special_tokens_map.json from cache at None\n",
            "loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--dbmdz--bert-base-turkish-cased/snapshots/bd38a3ecfe5d183400a573b1903e940d8d34902b/tokenizer_config.json\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--dbmdz--bert-base-turkish-cased/snapshots/bd38a3ecfe5d183400a573b1903e940d8d34902b/config.json\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"dbmdz/bert-base-turkish-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.26.0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32000\n",
            "}\n",
            "\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--dbmdz--bert-base-turkish-cased/snapshots/bd38a3ecfe5d183400a573b1903e940d8d34902b/config.json\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"dbmdz/bert-base-turkish-cased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.26.0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32000\n",
            "}\n",
            "\n"
          ]
        }
      ],
      "source": [
        "loaded_model = AutoModelForSequenceClassification.from_pretrained(loading_location)\n",
        "\n",
        "new_tokenizer = AutoTokenizer.from_pretrained(choosen_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YFB85Y_YkgyT"
      },
      "source": [
        "# Prediction\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yYvBe8Hfkjm8"
      },
      "outputs": [],
      "source": [
        "def get_prediction(text):\n",
        "    encoding = new_tokenizer(text, return_tensors=\"pt\", padding=\"max_length\", truncation=True, max_length=128)\n",
        "    encoding = {k: v.to(loaded_model.device) for k,v in encoding.items()}\n",
        "\n",
        "    outputs = loaded_model(**encoding)\n",
        "\n",
        "    logits = outputs.logits\n",
        "\n",
        "    sigmoid = torch.nn.Sigmoid()\n",
        "    probs = sigmoid(logits.squeeze().cpu())\n",
        "    probs = probs.detach().numpy()\n",
        "    label = np.argmax(probs, axis=-1)\n",
        "    \n",
        "    return label\n",
        "      "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QvZLKKnnk0fw",
        "outputId": "ac3bcfa0-863d-462d-966a-adf02edb2c6c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "get_prediction('Son dakika: Savunma Sanayisinden çok önemli proje! Ve imzalar atıldı')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HoS4P3KEk5A0"
      },
      "source": [
        "# Accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xRuuHajlk66G"
      },
      "outputs": [],
      "source": [
        "accuracy = evaluate.load(\"accuracy\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OO89woFYk9LG"
      },
      "outputs": [],
      "source": [
        "prediction  = []\n",
        "referance = []\n",
        "print(len(valid_df))\n",
        "\n",
        "for i in range(len(valid_df)):\n",
        "  var = valid_df.iloc[i][\"labels\"]\n",
        "  vartwo = get_prediction(valid_df.iloc[i][\"text\"])\n",
        "  print(i)\n",
        "\n",
        "  if var == 0 :\n",
        "    varthree = 0\n",
        "  else:\n",
        "    varthree = 1\n",
        "\n",
        "  referance.append(varthree)\n",
        "  prediction.append(vartwo)\n",
        "\n",
        "\n",
        "\n",
        "accuracy.add_batch(references=referance , predictions=prediction)\n",
        "\n",
        "accuracy.compute()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PPJficRgbDgb"
      },
      "outputs": [],
      "source": [
        "metrics.add_batch(references=referance , predictions=prediction)\n",
        "\n",
        "metrics.compute()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uuBOYraoqdtT"
      },
      "outputs": [],
      "source": [
        "for i in range(len(prediction)):\n",
        "  print(prediction[i],referance[i])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Azure Back Translation\n"
      ],
      "metadata": {
        "id": "22yNkicEdjlj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Add your key and endpoint\n",
        "key = \"\"\n",
        "endpoint = \"https://api.cognitive.microsofttranslator.com\"\n",
        "\n",
        "# location, also known as region.\n",
        "# required if you're using a multi-service or regional (not global) resource. It can be found in the Azure portal on the Keys and Endpoint page.\n",
        "location = \"westeurope\"\n",
        "\n",
        "path = '/translate'\n",
        "constructed_url = endpoint + path"
      ],
      "metadata": {
        "id": "MJeBLR3ddoAO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "params = {\n",
        "    'api-version': '3.0',\n",
        "    'from': 'tr',\n",
        "    'to': 'en'\n",
        "}\n",
        "\n",
        "headers = {\n",
        "    'Ocp-Apim-Subscription-Key': key,\n",
        "    # location required if you're using a multi-service or regional (not global) resource.\n",
        "    'Ocp-Apim-Subscription-Region': location,\n",
        "    'Content-type': 'application/json',\n",
        "    'X-ClientTraceId': str(uuid.uuid4())\n",
        "}"
      ],
      "metadata": {
        "id": "PjepRIuwdw1_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "request = requests.post(constructed_url, params=params, headers=headers, json=body)\n",
        "response = request.json() \n",
        "print(  json.dumps(response, sort_keys=True, ensure_ascii=False, indent=4, separators=(',', ': ')))"
      ],
      "metadata": {
        "id": "284Bd7UBd1l_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def TranslateText(text):\n",
        "  text = text.replace(\"\\n\",\" \")\n",
        "  body = [{'text':text}]\n",
        "  request = requests.post(constructed_url, params=params, headers=headers, json=body)\n",
        "  response = request.json() \n",
        "  return response[0][\"translations\"][0][\"text\"]"
      ],
      "metadata": {
        "id": "heRRB0drd3MQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Augmentation\n"
      ],
      "metadata": {
        "id": "cLurVLqbd4TO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Non model augmentations\n"
      ],
      "metadata": {
        "id": "fByUrnkNhhfl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sub_char_by_key = nac.KeyboardAug(aug_word_min=10, aug_word_max=30)"
      ],
      "metadata": {
        "id": "bmjX9jPXd7RE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "swap_char = nac.RandomCharAug(action=\"swap\", aug_word_min=10, aug_word_max=30)"
      ],
      "metadata": {
        "id": "jTt39bQugl0R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "delete_char = nac.RandomCharAug(action=\"delete\", aug_word_min=10, aug_word_max=30)"
      ],
      "metadata": {
        "id": "nKdTm0Izgo3J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "delete_word = naw.RandomWordAug(aug_min = 10, aug_max = 30)"
      ],
      "metadata": {
        "id": "F7JSHStpgqj-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "delete_set_words = naw.RandomWordAug(action='crop', aug_min = 10, aug_max = 30)"
      ],
      "metadata": {
        "id": "Iu6Is82JgsjA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "split_word = naw.SplitAug(aug_min = 10, aug_max = 30)"
      ],
      "metadata": {
        "id": "qdCBQ5qJgue8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "swap_words = naw.RandomWordAug(action=\"swap\",aug_min = 10, aug_max = 30)"
      ],
      "metadata": {
        "id": "6ybDwoIbokvA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def augmentation(df,augmentationModel):\n",
        "  fake_text=df[\"Orj_Text\"]\n",
        "  fake_text=fake_text.str.replace(\"\\n\", \" \")\n",
        "  texts = list(fake_text)\n",
        "  aug_list=[]\n",
        "  for i in range(len(texts)):\n",
        "    if len(texts[i].split()) <= 10 and augmentationModel == delete_set_words :\n",
        "      dsw_short= naw.RandomWordAug(action='crop', aug_min = 2)\n",
        "      augmented_text = dsw_short.augment(texts[i])\n",
        "    else:\n",
        "      augmented_text = augmentationModel.augment(texts[i])\n",
        "    aug_list.append(augmented_text)\n",
        "    print(\"Augmented Text:\")\n",
        "    print(augmented_text)\n",
        "  return aug_list"
      ],
      "metadata": {
        "id": "0NjOFOiBg3T7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Augmentations With Models\n"
      ],
      "metadata": {
        "id": "veJ36YsOiA0n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "insert_berturk = naw.ContextualWordEmbsAug(\n",
        "      model_path='dbmdz/bert-base-turkish-cased', action=\"insert\", aug_min = 10, aug_max = 30)\n",
        "insert_convberturk = naw.ContextualWordEmbsAug(\n",
        "      model_path='dbmdz/convbert-base-turkish-cased', action=\"insert\", aug_min = 10, aug_max = 30)\n",
        "\n",
        "augmentations=[insert_Berturk,insert_Convberturk]"
      ],
      "metadata": {
        "id": "1iC6uxOdiRp5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "substitute_berturk = naw.ContextualWordEmbsAug(\n",
        "      model_path='dbmdz/bert-base-turkish-cased', action=\"substitute\", aug_min = 10, aug_max = 30)\n",
        "substitute_convberturk = naw.ContextualWordEmbsAug(\n",
        "      model_path='dbmdz/convbert-base-turkish-cased', action=\"substitute\", aug_min = 10, aug_max = 30)\n",
        "\n",
        "augmentations=[substitute_berturk,substitute_convberturk]"
      ],
      "metadata": {
        "id": "vjXvjbHzoO_F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def augmentation(df):\n",
        "  fake_text=df[\"Orj_Text\"]\n",
        "  fake_text=fake_text.str.replace(\"\\n\", \" \")\n",
        "  texts = list(fake_text)\n",
        "  aug_list=[]\n",
        "  for j in range(len(augmentations)):\n",
        "    print(augmentations[j])\n",
        "    for i in range(len(texts)):\n",
        "      augmented_text = augmentations[j].augment(texts[i])\n",
        "      aug_list.append(augmented_text)\n",
        "      print(\"Augmented Text:\")\n",
        "      print(augmented_text)\n",
        "  return aug_list"
      ],
      "metadata": {
        "id": "dntIfXuamGtK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Stop Word Removal\n"
      ],
      "metadata": {
        "id": "cQ5D3BkbpZnQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#nltk.download()"
      ],
      "metadata": {
        "id": "V36KVgiYp9rr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "example_sent = \"\"\"This is a sample sentence,\n",
        "                  showing off the stop words filtration.\"\"\"\n",
        "  \n",
        "stop_words = set(stopwords.words('english'))\n",
        "  \n",
        "word_tokens = word_tokenize(example_sent)\n",
        "# converts the words in word_tokens to lower case and then checks whether \n",
        "#they are present in stop_words or not\n",
        "filtered_sentence = [w for w in word_tokens if not w.lower() in stop_words]\n",
        "#with no lower case conversion\n",
        "filtered_sentence = []\n",
        "  \n",
        "for w in word_tokens:\n",
        "    if w not in stop_words:\n",
        "        filtered_sentence.append(w)\n",
        "  \n",
        "print(word_tokens)\n",
        "print(filtered_sentence)"
      ],
      "metadata": {
        "id": "mdudY-BOpgX3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_stop_words (text):\n",
        "  word_tokens = word_tokenize(text)\n",
        "  filtered_sentence = [w for w in word_tokens if not w.lower() in stop_words]\n",
        "  result = \"\"\n",
        "  for w in word_tokens:\n",
        "    if w not in stop_words:\n",
        "        result += w\n",
        "        result +=  \" \"\n",
        "  return result "
      ],
      "metadata": {
        "id": "CaQ5Mtnyprcm"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "k22fnkk9f-Js",
        "epaoMhSViA4F",
        "enlJ4sTVgI9W",
        "zDEBimJhha16",
        "-uYQgemKgSbD",
        "IDm-QWI1jcqP",
        "wwfNf4vSjtLK",
        "lSyIv6kIj3LY",
        "BJE8Bc25kPgi",
        "YFB85Y_YkgyT",
        "HoS4P3KEk5A0",
        "fByUrnkNhhfl"
      ],
      "machine_shape": "hm",
      "provenance": [],
      "gpuClass": "premium",
      "include_colab_link": true
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ilkayyagizgur/Fake_News_Training/blob/main/Fake_News_Training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k22fnkk9f-Js"
      },
      "source": [
        "# Download and Import Required Libraries\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall -y transformers accelerate\n",
        "!pip install transformers accelerate"
      ],
      "metadata": {
        "id": "Skjq-Hm1A0Yt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z7W7dPhloq1t"
      },
      "outputs": [],
      "source": [
        "!pip install transformers\n",
        "!pip install datasets\n",
        "!pip install numpy\n",
        "!pip install evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "McjPNxWUqksI"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForSequenceClassification\n",
        "from transformers import TrainingArguments, Trainer\n",
        "from transformers import AutoTokenizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from datasets import Dataset\n",
        "import pyarrow as pa\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import evaluate\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "doWGJgStfsPP",
        "outputId": "319aa257-dd4f-4aeb-b157-f0a61aeef233"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "epaoMhSViA4F"
      },
      "source": [
        "# Model Settings\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UsuMsfOppjT-"
      },
      "outputs": [],
      "source": [
        "choosen_model ='dbmdz/bert-base-turkish-cased'\n",
        "number_labels = 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0JLURHyyiE4a"
      },
      "outputs": [],
      "source": [
        "dataset_location ='/content/drive/MyDrive/Bitirme/LastDatasetWithAllAug.csv'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CorMCqxvl7Qw"
      },
      "outputs": [],
      "source": [
        "trainer_output_dir = \"/content/drive/MyDrive/Berturk-cased-model\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "66LJW-SAlnsq"
      },
      "outputs": [],
      "source": [
        "saving_location = '/content/drive/MyDrive/Bert/Models/Berturk-cased-model'\n",
        "loading_location = saving_location"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "enlJ4sTVgI9W"
      },
      "source": [
        "# Choose model and Download\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UmMqbFbnqVUK"
      },
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(choosen_model)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    choosen_model,\n",
        "    num_labels=number_labels\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G3CpKBUbhQFq"
      },
      "outputs": [],
      "source": [
        "metrics = evaluate.combine([\"accuracy\", \"f1\", \"precision\", \"recall\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zDEBimJhha16"
      },
      "source": [
        "# Defining Required Functions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-J4CLweAhg3i"
      },
      "outputs": [],
      "source": [
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    return metrics.compute(predictions=predictions, references=labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4i-hpCuhhiAL"
      },
      "outputs": [],
      "source": [
        "def process_data(row):\n",
        "\n",
        "    text = row['Orj_Text']\n",
        "    text = str(text)\n",
        "    text = ' '.join(text.split())\n",
        "\n",
        "    encodings = tokenizer(text,\n",
        "                          padding=\"max_length\",\n",
        "                          truncation=True,\n",
        "                          max_length=128)\n",
        "\n",
        "    label = 0\n",
        "    if row['label'] == 1:\n",
        "        label += 1\n",
        "\n",
        "    encodings['labels'] = label\n",
        "    encodings['text'] = text\n",
        "\n",
        "    return encodings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-uYQgemKgSbD"
      },
      "source": [
        "# Import and Procces Dataset\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZGCG5G8SqVhr"
      },
      "outputs": [],
      "source": [
        "dataset = pd.read_csv(dataset_location)\n",
        "dataset.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gQlzPgY-ikDM"
      },
      "outputs": [],
      "source": [
        "processed_dataset = []\n",
        "\n",
        "for i in range(len(dataset[:20037])):\n",
        "    processed_dataset.append(process_data(dataset.iloc[i]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zlbTLVZcSnQW"
      },
      "outputs": [],
      "source": [
        "processed_dataset[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HhAC4hl-ipTi"
      },
      "outputs": [],
      "source": [
        "new_df = pd.DataFrame(processed_dataset)\n",
        "\n",
        "train_df, valid_df = train_test_split(\n",
        "    new_df,\n",
        "    test_size=0.2,\n",
        "    random_state=42\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_MHZHWCHjL4Z"
      },
      "outputs": [],
      "source": [
        "train_dataset = Dataset(pa.Table.from_pandas(train_df))\n",
        "valid_dataset = Dataset(pa.Table.from_pandas(valid_df))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IDm-QWI1jcqP"
      },
      "source": [
        "# Setting Up Trainer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OGnoBt1_jf1T"
      },
      "outputs": [],
      "source": [
        "training_args = TrainingArguments(output_dir=trainer_output_dir, evaluation_strategy=\"epoch\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PLCN1qVajh4c"
      },
      "outputs": [],
      "source": [
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=valid_dataset,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wwfNf4vSjtLK"
      },
      "source": [
        "# Train and Evaluate\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BAZb5U2cjx1k"
      },
      "outputs": [],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2wJbxaXIjxkG"
      },
      "outputs": [],
      "source": [
        "trainer.evaluate()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lSyIv6kIj3LY"
      },
      "source": [
        "# Save Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_nueQa25j6BY"
      },
      "outputs": [],
      "source": [
        "model.save_pretrained(saving_location)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.save_pretrained(saving_location, legacy_format=False)"
      ],
      "metadata": {
        "id": "8vs-8XzxCLDV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BJE8Bc25kPgi"
      },
      "source": [
        "# Load Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QFkZwtcgkRYk"
      },
      "outputs": [],
      "source": [
        "loaded_model = AutoModelForSequenceClassification.from_pretrained(loading_location)\n",
        "\n",
        "new_tokenizer = AutoTokenizer.from_pretrained(choosen_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YFB85Y_YkgyT"
      },
      "source": [
        "# Prediction\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yYvBe8Hfkjm8"
      },
      "outputs": [],
      "source": [
        "def get_prediction(text):\n",
        "    encoding = new_tokenizer(text, return_tensors=\"pt\", padding=\"max_length\", truncation=True, max_length=128)\n",
        "    encoding = {k: v.to(loaded_model.device) for k,v in encoding.items()}\n",
        "\n",
        "    outputs = loaded_model(**encoding)\n",
        "\n",
        "    logits = outputs.logits\n",
        "\n",
        "    sigmoid = torch.nn.Sigmoid()\n",
        "    probs = sigmoid(logits.squeeze().cpu())\n",
        "    probs = probs.detach().numpy()\n",
        "    label = np.argmax(probs, axis=-1)\n",
        "    \n",
        "    return label\n",
        "      "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QvZLKKnnk0fw"
      },
      "outputs": [],
      "source": [
        "get_prediction('Son dakika: Savunma Sanayisinden çok önemli proje! Ve imzalar atıldı')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HoS4P3KEk5A0"
      },
      "source": [
        "# Accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xRuuHajlk66G"
      },
      "outputs": [],
      "source": [
        "accuracy = evaluate.load(\"accuracy\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OO89woFYk9LG"
      },
      "outputs": [],
      "source": [
        "prediction  = []\n",
        "referance = []\n",
        "print(len(valid_df))\n",
        "\n",
        "for i in range(len(valid_df)):\n",
        "  var = valid_df.iloc[i][\"labels\"]\n",
        "  vartwo = get_prediction(valid_df.iloc[i][\"text\"])\n",
        "  print(i)\n",
        "\n",
        "  if var == 0 :\n",
        "    varthree = 0\n",
        "  else:\n",
        "    varthree = 1\n",
        "\n",
        "  referance.append(varthree)\n",
        "  prediction.append(vartwo)\n",
        "\n",
        "\n",
        "\n",
        "accuracy.add_batch(references=referance , predictions=prediction)\n",
        "\n",
        "accuracy.compute()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PPJficRgbDgb"
      },
      "outputs": [],
      "source": [
        "metrics.add_batch(references=referance , predictions=prediction)\n",
        "\n",
        "metrics.compute()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uuBOYraoqdtT"
      },
      "outputs": [],
      "source": [
        "for i in range(len(prediction)):\n",
        "  print(prediction[i],referance[i])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Azure Back Translation\n"
      ],
      "metadata": {
        "id": "22yNkicEdjlj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Add your key and endpoint\n",
        "key = \"\"\n",
        "endpoint = \"https://api.cognitive.microsofttranslator.com\"\n",
        "\n",
        "# location, also known as region.\n",
        "# required if you're using a multi-service or regional (not global) resource. It can be found in the Azure portal on the Keys and Endpoint page.\n",
        "location = \"westeurope\"\n",
        "\n",
        "path = '/translate'\n",
        "constructed_url = endpoint + path"
      ],
      "metadata": {
        "id": "MJeBLR3ddoAO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "params = {\n",
        "    'api-version': '3.0',\n",
        "    'from': 'tr',\n",
        "    'to': 'en'\n",
        "}\n",
        "\n",
        "headers = {\n",
        "    'Ocp-Apim-Subscription-Key': key,\n",
        "    # location required if you're using a multi-service or regional (not global) resource.\n",
        "    'Ocp-Apim-Subscription-Region': location,\n",
        "    'Content-type': 'application/json',\n",
        "    'X-ClientTraceId': str(uuid.uuid4())\n",
        "}"
      ],
      "metadata": {
        "id": "PjepRIuwdw1_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "request = requests.post(constructed_url, params=params, headers=headers, json=body)\n",
        "response = request.json() \n",
        "print(  json.dumps(response, sort_keys=True, ensure_ascii=False, indent=4, separators=(',', ': ')))"
      ],
      "metadata": {
        "id": "284Bd7UBd1l_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def TranslateText(text):\n",
        "  text = text.replace(\"\\n\",\" \")\n",
        "  body = [{'text':text}]\n",
        "  request = requests.post(constructed_url, params=params, headers=headers, json=body)\n",
        "  response = request.json() \n",
        "  return response[0][\"translations\"][0][\"text\"]"
      ],
      "metadata": {
        "id": "heRRB0drd3MQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Augmentation\n"
      ],
      "metadata": {
        "id": "cLurVLqbd4TO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Non model augmentations\n"
      ],
      "metadata": {
        "id": "fByUrnkNhhfl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sub_char_by_key = nac.KeyboardAug(aug_word_min=10, aug_word_max=30)"
      ],
      "metadata": {
        "id": "bmjX9jPXd7RE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "swap_char = nac.RandomCharAug(action=\"swap\", aug_word_min=10, aug_word_max=30)"
      ],
      "metadata": {
        "id": "jTt39bQugl0R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "delete_char = nac.RandomCharAug(action=\"delete\", aug_word_min=10, aug_word_max=30)"
      ],
      "metadata": {
        "id": "nKdTm0Izgo3J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "delete_word = naw.RandomWordAug(aug_min = 10, aug_max = 30)"
      ],
      "metadata": {
        "id": "F7JSHStpgqj-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "delete_set_words = naw.RandomWordAug(action='crop', aug_min = 10, aug_max = 30)"
      ],
      "metadata": {
        "id": "Iu6Is82JgsjA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "split_word = naw.SplitAug(aug_min = 10, aug_max = 30)"
      ],
      "metadata": {
        "id": "qdCBQ5qJgue8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "swap_words = naw.RandomWordAug(action=\"swap\",aug_min = 10, aug_max = 30)"
      ],
      "metadata": {
        "id": "6ybDwoIbokvA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def augmentation(df,augmentationModel):\n",
        "  fake_text=df[\"Orj_Text\"]\n",
        "  fake_text=fake_text.str.replace(\"\\n\", \" \")\n",
        "  texts = list(fake_text)\n",
        "  aug_list=[]\n",
        "  for i in range(len(texts)):\n",
        "    if len(texts[i].split()) <= 10 and augmentationModel == delete_set_words :\n",
        "      dsw_short= naw.RandomWordAug(action='crop', aug_min = 2)\n",
        "      augmented_text = dsw_short.augment(texts[i])\n",
        "    else:\n",
        "      augmented_text = augmentationModel.augment(texts[i])\n",
        "    aug_list.append(augmented_text)\n",
        "    print(\"Augmented Text:\")\n",
        "    print(augmented_text)\n",
        "  return aug_list"
      ],
      "metadata": {
        "id": "0NjOFOiBg3T7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Augmentations With Models\n"
      ],
      "metadata": {
        "id": "veJ36YsOiA0n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "insert_berturk = naw.ContextualWordEmbsAug(\n",
        "      model_path='dbmdz/bert-base-turkish-cased', action=\"insert\", aug_min = 10, aug_max = 30)\n",
        "insert_convberturk = naw.ContextualWordEmbsAug(\n",
        "      model_path='dbmdz/convbert-base-turkish-cased', action=\"insert\", aug_min = 10, aug_max = 30)\n",
        "\n",
        "augmentations=[insert_Berturk,insert_Convberturk]"
      ],
      "metadata": {
        "id": "1iC6uxOdiRp5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "substitute_berturk = naw.ContextualWordEmbsAug(\n",
        "      model_path='dbmdz/bert-base-turkish-cased', action=\"substitute\", aug_min = 10, aug_max = 30)\n",
        "substitute_convberturk = naw.ContextualWordEmbsAug(\n",
        "      model_path='dbmdz/convbert-base-turkish-cased', action=\"substitute\", aug_min = 10, aug_max = 30)\n",
        "\n",
        "augmentations=[substitute_berturk,substitute_convberturk]"
      ],
      "metadata": {
        "id": "vjXvjbHzoO_F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def augmentation(df):\n",
        "  fake_text=df[\"Orj_Text\"]\n",
        "  fake_text=fake_text.str.replace(\"\\n\", \" \")\n",
        "  texts = list(fake_text)\n",
        "  aug_list=[]\n",
        "  for j in range(len(augmentations)):\n",
        "    print(augmentations[j])\n",
        "    for i in range(len(texts)):\n",
        "      augmented_text = augmentations[j].augment(texts[i])\n",
        "      aug_list.append(augmented_text)\n",
        "      print(\"Augmented Text:\")\n",
        "      print(augmented_text)\n",
        "  return aug_list"
      ],
      "metadata": {
        "id": "dntIfXuamGtK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Stop Word Removal\n"
      ],
      "metadata": {
        "id": "cQ5D3BkbpZnQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#nltk.download()"
      ],
      "metadata": {
        "id": "V36KVgiYp9rr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "example_sent = \"\"\"This is a sample sentence,\n",
        "                  showing off the stop words filtration.\"\"\"\n",
        "  \n",
        "stop_words = set(stopwords.words('english'))\n",
        "  \n",
        "word_tokens = word_tokenize(example_sent)\n",
        "# converts the words in word_tokens to lower case and then checks whether \n",
        "#they are present in stop_words or not\n",
        "filtered_sentence = [w for w in word_tokens if not w.lower() in stop_words]\n",
        "#with no lower case conversion\n",
        "filtered_sentence = []\n",
        "  \n",
        "for w in word_tokens:\n",
        "    if w not in stop_words:\n",
        "        filtered_sentence.append(w)\n",
        "  \n",
        "print(word_tokens)\n",
        "print(filtered_sentence)"
      ],
      "metadata": {
        "id": "mdudY-BOpgX3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_stop_words (text):\n",
        "  word_tokens = word_tokenize(text)\n",
        "  filtered_sentence = [w for w in word_tokens if not w.lower() in stop_words]\n",
        "  result = \"\"\n",
        "  for w in word_tokens:\n",
        "    if w not in stop_words:\n",
        "        result += w\n",
        "        result +=  \" \"\n",
        "  return result "
      ],
      "metadata": {
        "id": "CaQ5Mtnyprcm"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "k22fnkk9f-Js",
        "epaoMhSViA4F",
        "enlJ4sTVgI9W",
        "zDEBimJhha16",
        "-uYQgemKgSbD",
        "IDm-QWI1jcqP",
        "wwfNf4vSjtLK",
        "lSyIv6kIj3LY",
        "BJE8Bc25kPgi",
        "YFB85Y_YkgyT",
        "HoS4P3KEk5A0",
        "fByUrnkNhhfl"
      ],
      "machine_shape": "hm",
      "provenance": [],
      "gpuClass": "premium",
      "include_colab_link": true
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}